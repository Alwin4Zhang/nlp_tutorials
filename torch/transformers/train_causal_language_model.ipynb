{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_causal_language_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO1+plzG4FCQ6cLD5+12oB6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"thSP3RPsGvA1","executionInfo":{"status":"ok","timestamp":1655041045262,"user_tz":-480,"elapsed":8021,"user":{"displayName":"张俊峰","userId":"00138305159102024959"}},"outputId":"e5a7c463-c87c-4297-e06a-c18e783cd125"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.2.2)\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.19.4)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.7.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.5.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: dill<0.3.5 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.5.18.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.96)\n","Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.9.0)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.11.0+cu113)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.21.6)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (4.2.0)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","git-lfs is already the newest version (2.3.4-1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"]}],"source":["!pip install datasets transformers[sentencepiece]\n","!pip install accelerate\n","# To run the training on TPU, you will need to uncomment the followin line:\n","# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n","!apt install git-lfs"]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2E8ya-UHHaYS","executionInfo":{"status":"ok","timestamp":1655041058303,"user_tz":-480,"elapsed":13048,"user":{"displayName":"张俊峰","userId":"00138305159102024959"}},"outputId":"a7ad52cd-6ba8-4a9b-c98a-f1992c3d3e3e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens .\n","        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n","        \n","Token: \n","Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}]},{"cell_type":"code","source":["def any_keyword_in_string(string, keywords):\n","    for keyword in keywords:\n","        if keyword in string:\n","            return True\n","    return False"],"metadata":{"id":"EdIsfZtHH18X","executionInfo":{"status":"ok","timestamp":1655041058306,"user_tz":-480,"elapsed":27,"user":{"displayName":"张俊峰","userId":"00138305159102024959"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n","example_1 = \"import numpy as np\"\n","example_2 = \"import pandas as pd\"\n","\n","print(\n","    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7UhNsmbIBtX","executionInfo":{"status":"ok","timestamp":1655041058309,"user_tz":-480,"elapsed":26,"user":{"displayName":"张俊峰","userId":"00138305159102024959"}},"outputId":"b3167dc5-7615-4b7b-c4aa-1a470f4e06e6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["False True\n"]}]},{"cell_type":"code","source":["from collections import defaultdict\n","from tqdm import tqdm\n","from datasets import Dataset\n","\n","def filter_streaming_dataset(dataset,filters):\n","  filtered_dict = defaultdict(list)\n","  total = 0\n","  for sample in tqdm(iter(dataset)):\n","    total += 1\n","    if any_keyword_in_string(sample['content'],filters):\n","      for k,v in sample.items():\n","        filtered_dict[k].append(v)\n","  print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n","  return Dataset.from_dict(filtered_dict)"],"metadata":{"id":"RhCW5cGTIEz5","executionInfo":{"status":"ok","timestamp":1655041062203,"user_tz":-480,"elapsed":3915,"user":{"displayName":"张俊峰","userId":"00138305159102024959"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# This cell will take a very long time to execute, so you should skip it and go to\n","# the next one!\n","from datasets import load_dataset\n","\n","split = \"train\"  # \"valid\"\n","filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n","\n","data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n","filtered_data = filter_streaming_dataset(data, filters)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtfGjnNBIedM","outputId":"71fa857a-e029-42f2-8f97-f307b562b774"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using custom data configuration transformersbook--codeparrot-train-39df86ab4d07ce93\n","1711977it [22:49, 2112.49it/s]"]}]},{"cell_type":"code","source":["from datasets import load_dataset, DatasetDict\n","\n","ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n","ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n","\n","raw_datasets = DatasetDict(\n","    {\n","        \"train\": ds_train,  # .shuffle().select(range(50000)),\n","        \"valid\": ds_valid,  # .shuffle().select(range(500))\n","    }\n",")\n","\n","raw_datasets"],"metadata":{"id":"jyeWrxOlI2Ep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for key in raw_datasets[\"train\"][0]:\n","    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"],"metadata":{"id":"s4BMoMz2J5mg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","context_length = 128\n","tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n","\n","outputs = tokenizer(\n","    raw_datasets[\"train\"][:2][\"content\"],\n","    truncation=True,\n","    max_length=context_length,\n","    return_overflowing_tokens=True,\n","    return_length=True,\n",")\n","\n","print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n","print(f\"Input chunk lengths: {(outputs['length'])}\")\n","print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"],"metadata":{"id":"lCsKGSGFJ7dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize(element):\n","  outputs = tokenizer(\n","      element[\"content\"],\n","      truncation=True,\n","      max_length = context_length,\n","      return_overflowing_tokens = True,\n","      return_length = True\n","  )\n","  input_batch = []\n","  for length,input_ids in zip(outputs['length'],outputs['input_ids']):\n","    if length == context_length:\n","      input_batch.append(input_ids)\n","  return {'input_ids':input_batch}"],"metadata":{"id":"-Gs6rEpIJ_mY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_datasets = raw_datasets.map(\n","    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",")\n","tokenized_datasets"],"metadata":{"id":"2cwKbYqoI4WC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n","\n","config = AutoConfig.from_pretrained(\n","    \"gpt2\",\n","    vocab_size=len(tokenizer),\n","    n_ctx=context_length,\n","    bos_token_id=tokenizer.bos_token_id,\n","    eos_token_id=tokenizer.eos_token_id,\n",")"],"metadata":{"id":"ChD3_MR8JgUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GPT2LMHeadModel(config)\n","model_size = sum(t.numel() for t in model.parameters())\n","print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"],"metadata":{"id":"2yNL9oLEJkns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorForLanguageModeling\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"],"metadata":{"id":"F_xtsOUiJ0Jj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n","for key in out:\n","    print(f\"{key} shape: {out[key].shape}\")"],"metadata":{"id":"uXLFtxeYKAmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","args = TrainingArguments(\n","    output_dir=\"codeparrot-ds\",\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=5_000,\n","    logging_steps=5_000,\n","    gradient_accumulation_steps=8,\n","    num_train_epochs=1,\n","    weight_decay=0.1,\n","    warmup_steps=1_000,\n","    lr_scheduler_type=\"cosine\",\n","    learning_rate=5e-4,\n","    save_steps=5_000,\n","    fp16=True\n",")"],"metadata":{"id":"blzTQh3DM0HB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    args=args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"valid\"],\n",")"],"metadata":{"id":"F15pc6C4M4mT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"JsOsUeyRM7K2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import pipeline\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","pipe = pipeline(\n","    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n",")"],"metadata":{"id":"ihMR2eC6NN2W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["txt = \"\"\"\\\n","# create some data\n","x = np.random.randn(100)\n","y = np.random.randn(100)\n","\n","# create scatter plot with x, y\n","\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"],"metadata":{"id":"hcJrs8mXNQKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["txt = \"\"\"\\\n","# create some data\n","x = np.random.randn(100)\n","y = np.random.randn(100)\n","\n","# create dataframe from x and y\n","\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"],"metadata":{"id":"Wy7-4WX-NS8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["txt = \"\"\"\\\n","# dataframe with profession, income and name\n","df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n","\n","# calculate the mean income per profession\n","\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"],"metadata":{"id":"MMbQ6IHHNVgl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["txt = \"\"\"\n","# import random forest regressor from scikit-learn\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# fit random forest model with 300 estimators on X, y:\n","\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"],"metadata":{"id":"ZTxgRWQLNd9g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keytoken_ids = []\n","for keyword in [\n","    \"plt\",\n","    \"pd\",\n","    \"sk\",\n","    \"fit\",\n","    \"predict\",\n","    \" plt\",\n","    \" pd\",\n","    \" sk\",\n","    \" fit\",\n","    \" predict\",\n","    \"testtest\",\n","]:\n","    ids = tokenizer([keyword]).input_ids[0]\n","    if len(ids) == 1:\n","        keytoken_ids.append(ids[0])\n","    else:\n","        print(f\"Keyword has not single token: {keyword}\")"],"metadata":{"id":"XZW0VtNmNflE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn import CrossEntropyLoss\n","import torch\n","\n","\n","def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n","    # Shift so that tokens < n predict n\n","    shift_labels = inputs[..., 1:].contiguous()\n","    shift_logits = logits[..., :-1, :].contiguous()\n","    # Calculate per-token loss\n","    loss_fct = CrossEntropyLoss(reduce=False)\n","    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","    # Resize and average loss per sample\n","    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n","    # Calculate and scale weighting\n","    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n","        axis=[0, 2]\n","    )\n","    weights = alpha * (1.0 + weights)\n","    # Calculate weighted average\n","    weighted_loss = (loss_per_sample * weights).mean()\n","    return weighted_loss"],"metadata":{"id":"vEw8T-jINjBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data.dataloader import DataLoader\n","\n","tokenized_dataset.set_format(\"torch\")\n","train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=32, shuffle=True)\n","eval_dataloader = DataLoader(tokenized_dataset[\"valid\"], batch_size=32)"],"metadata":{"id":"DntJa-ZsNkyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weight_decay = 0.1\n","\n","\n","def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n","    params_with_wd, params_without_wd = [], []\n","    for n, p in model.named_parameters():\n","        if any(nd in n for nd in no_decay):\n","            params_without_wd.append(p)\n","        else:\n","            params_with_wd.append(p)\n","    return [\n","        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n","        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n","    ]"],"metadata":{"id":"dCslMvyjOT1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate():\n","    model.eval()\n","    losses = []\n","    for step, batch in enumerate(eval_dataloader):\n","        with torch.no_grad():\n","            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n","\n","        losses.append(accelerator.gather(outputs.loss))\n","    loss = torch.mean(torch.cat(losses))\n","    try:\n","        perplexity = torch.exp(loss)\n","    except OverflowError:\n","        perplexity = float(\"inf\")\n","    return loss.item(), perplexity.item()"],"metadata":{"id":"ftxwbo2uOVxE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GPT2LMHeadModel(config)"],"metadata":{"id":"eIiE1kcDOX6q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim import AdamW\n","\n","optimizer = AdamW(get_grouped_params(model), lr=5e-4)"],"metadata":{"id":"sdeB_k_YOZw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from accelerate import Accelerator\n","\n","accelerator = Accelerator(fp16=True)\n","\n","model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n","    model, optimizer, train_dataloader, eval_dataloader\n",")"],"metadata":{"id":"H1LRyXOvObbH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import get_scheduler\n","\n","num_train_epochs = 1\n","num_update_steps_per_epoch = len(train_dataloader)\n","num_training_steps = num_train_epochs * num_update_steps_per_epoch\n","\n","lr_scheduler = get_scheduler(\n","    name=\"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=1_000,\n","    num_training_steps=num_training_steps,\n",")"],"metadata":{"id":"dykhKg9xOdGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","\n","output_dir = \"codeparrot-ds-accelerate\"\n","\n","gradient_accumulation_steps = 8\n","eval_steps = 5_000\n","\n","model.train()\n","completed_steps = 0\n","for epoch in range(num_train_epochs):\n","    for step, batch in tqdm(\n","        enumerate(train_dataloader, start=1), total=num_training_steps\n","    ):\n","        logits = model(batch[\"input_ids\"]).logits\n","        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n","        if step % 100 == 0:\n","            accelerator.print(\n","                {\n","                    \"lr\": get_lr(),\n","                    \"samples\": step * samples_per_step,\n","                    \"steps\": completed_steps,\n","                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n","                }\n","            )\n","        loss = loss / gradient_accumulation_steps\n","        accelerator.backward(loss)\n","        if step % gradient_accumulation_steps == 0:\n","            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","            completed_steps += 1\n","        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n","            eval_loss, perplexity = evaluate()\n","            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n","            model.train()\n","            accelerator.wait_for_everyone()\n","            unwrapped_model = accelerator.unwrap_model(model)\n","            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n","            if accelerator.is_main_process:\n","                tokenizer.save_pretrained(output_dir)"],"metadata":{"id":"17UW1YIjOfCX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"wqV_9UROOqe7"},"execution_count":null,"outputs":[]}]}